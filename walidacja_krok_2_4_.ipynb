{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f89ee0b",
   "metadata": {},
   "source": [
    "# Walidacja drugiego kamienia milowego\n",
    "### Walidujący: Alicja Charuza, Mateusz Gałęziewski\n",
    "### Walidowana grupa: Maciej Borkowski, Michał Chęć\n",
    "\n",
    "Formą walidacji drugiego kamienia milowego będzie odpalenie dotychczas napisanego kodu na zbiorze testowym, a także sprawdzenie, jak przygotowane modele sobie na nim radzą."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b21c023b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ładujemy potrzebne pakiety\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tabulate import tabulate\n",
    "\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer, make_column_selector\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, MinMaxScaler\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, roc_auc_score, accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, train_test_split\n",
    "from sklearn.feature_selection import SelectKBest, SelectFromModel, SequentialFeatureSelector\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import warnings\n",
    "import os\n",
    "import sys\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    os.environ[\"PYTHONWARNINGS\"] = \"ignore\"\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f54caf1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wczytujemy całą ramkę danych\n",
    "dataset = pd.read_csv(\"dataset_1.csv\")\n",
    "df = dataset.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "94ce371b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dzielimy dane, tworzymy zbiór do ewaluacji i do testów\n",
    "train_set, eval_set = train_test_split(df, test_size=0.3, random_state=42)\n",
    "train_df, test_df = train_test_split(train_set, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "48b19d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# klasa do preprocessingu danych\n",
    "class ColumnRemover(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, threshold_constant, threshold_corr, n_info_vals):\n",
    "        self.threshold_constant = threshold_constant\n",
    "        self.threshold_corr = threshold_corr\n",
    "        self.n_info_vals = n_info_vals\n",
    "        self.columns_to_remove = []\n",
    "        self.columns_to_keep = []\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # usuwanie zduplikowanych kolumn\n",
    "        self.columns_to_remove.extend(X.loc[:, X.T.duplicated()].columns.tolist())\n",
    "\n",
    "        # usuwanie stałych i prawie stałych kolumn\n",
    "        for column in X.columns:\n",
    "            if X[column].value_counts(normalize=True).iloc[0] >= self.threshold_constant:\n",
    "                self.columns_to_remove.append(column)\n",
    "\n",
    "        # usuwanie skorelowanych kolumn\n",
    "        corr = X.corr()\n",
    "        corr = corr[corr > self.threshold_corr]\n",
    "        dependent_columns = corr.apply(lambda row: row[row > 0].index, axis=1)\n",
    "        for j in range (len(dependent_columns)):\n",
    "            for k in dependent_columns[j]:\n",
    "                if k is not dependent_columns.index[j]:\n",
    "                    if k not in dependent_columns.index[0:j]:\n",
    "                        self.columns_to_remove.append(k)\n",
    "\n",
    "        # usuwanie kolumn nie niosących informacji\n",
    "        amount_of_ones = y[y == 1].shape[0]\n",
    "        X = X.join(y)\n",
    "        for column in X.columns:\n",
    "            tmp = X.groupby(column)['target'].agg(['sum','count']).sort_values('sum',ascending = False).reset_index()\n",
    "            if any(tmp[column] == 0) and (tmp.loc[tmp[column] == 0, 'sum'] > amount_of_ones - self.n_info_vals).bool():\n",
    "                self.columns_to_remove.append(column)\n",
    "        X.drop('target', axis=1, inplace=True)\n",
    "\n",
    "        self.columns_to_keep = [col for col in X.columns if col not in self.columns_to_remove]\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X[self.columns_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f5eb963a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# zarówno dla zmiennych dyskretnych i ciągłych stosujemy nasz transformator ColumnRemover z różnymi parametrami - w kolejnych krokach będziemy szukać najlepszej ich kombinacji\n",
    "\n",
    "# dokonujemy kodowania one hot encoding zmiennych dyskretnych - traktujemy je jako kategoryczne\n",
    "int_transformer = Pipeline([\n",
    "    ('int', ColumnRemover(0.9995, 0.99, 1)),\n",
    "    ('one_hot', OneHotEncoder(handle_unknown='ignore', sparse=False, dtype='int64'))])\n",
    "\n",
    "# dokojumeny standaryzacji zmiennych ciągłych - w kolejnych krokach sprawdzimy, czy jest lepsza od normalizacji min_max\n",
    "float_transformer = Pipeline([\n",
    "    ('float', ColumnRemover(0.9999, 0.99, 10)),\n",
    "    ('min_max', StandardScaler())])\n",
    "\n",
    "col_transformer = ColumnTransformer([\n",
    "    ('int_pipe', int_transformer, make_column_selector(dtype_include=np.int64)),\n",
    "    ('float_pipe', float_transformer, make_column_selector(dtype_include=np.float64))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "729bd9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_df.drop('target', axis=1)\n",
    "y_train = train_df.target\n",
    "x_eval = eval_set.drop('target', axis=1)\n",
    "y_eval = eval_set.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fb8b5f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_scores(clf, X, y):\n",
    "    y_pred = clf.predict(X)\n",
    "    y_pred_prob = clf.predict_proba(X)\n",
    "    print(tabulate(confusion_matrix(y, y_pred), headers=['Predicted 0', 'Predicted 1'], tablefmt='orgtbl'))\n",
    "    print()\n",
    "    print(f'accuracy:              {round(accuracy_score(y, y_pred), 4)}')\n",
    "    print(f'precision:             {round(precision_score(y, y_pred), 4)}')\n",
    "    print(f'recall:                {round(recall_score(y, y_pred), 4)}')\n",
    "    print(f'f1:                    {round(f1_score(y, y_pred), 4)}')\n",
    "    print(f'roc_auc_discrete:      {round(roc_auc_score(y, y_pred), 4)}')\n",
    "    print(f'roc_auc_continuous:    {round(roc_auc_score(y, y_pred_prob[:, 1]), 4)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081f9624",
   "metadata": {},
   "source": [
    "## Regresja logistyczna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e8e4dcbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   Predicted 0 |   Predicted 1 |\n",
      "|---------------+---------------|\n",
      "|         16315 |          7245 |\n",
      "|           209 |           731 |\n",
      "\n",
      "accuracy:              0.6958\n",
      "precision:             0.0916\n",
      "recall:                0.7777\n",
      "f1:                    0.164\n",
      "roc_auc_discrete:      0.7351\n",
      "roc_auc_continuous:    0.8131\n"
     ]
    }
   ],
   "source": [
    "clf = Pipeline([\n",
    "    ('preprocessing', col_transformer),\n",
    "    ('model', LogisticRegression(random_state=42, class_weight='balanced'))])\n",
    "\n",
    "clf.fit(x_train, y_train)\n",
    "show_scores(clf, x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "97e8ebc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   Predicted 0 |   Predicted 1 |\n",
      "|---------------+---------------|\n",
      "|          9888 |          4511 |\n",
      "|           166 |           435 |\n",
      "\n",
      "accuracy:              0.6882\n",
      "precision:             0.0879\n",
      "recall:                0.7238\n",
      "f1:                    0.1568\n",
      "roc_auc_discrete:      0.7053\n",
      "roc_auc_continuous:    0.7837\n"
     ]
    }
   ],
   "source": [
    "show_scores(clf, x_eval, y_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41d7895",
   "metadata": {},
   "source": [
    "Model regresji logistycznej ma problem z uczeniem się. Pomóc może zastowanie dodatkowych parametrów i redukcja kolumn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3123f0",
   "metadata": {},
   "source": [
    "## Drzewo decyzyjne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "61e92945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# zarówno dla zmiennych dyskretnych i ciągłych stosujemy nasz transformator ColumnRemover z różnymi parametrami - w kolejnych krokach będziemy szukać najlepszej ich kombinacji\n",
    "\n",
    "# dokonujemy kodowania one hot encoding zmiennych dyskretnych - traktujemy je jako kategoryczne\n",
    "int_transformer = Pipeline([\n",
    "    ('int', ColumnRemover(0.9995, 0.99, 1)),\n",
    "    ('one_hot', OneHotEncoder(handle_unknown='ignore', sparse=False, dtype='int64'))])\n",
    "\n",
    "# jesteśmy przy drzewach decyzyjnych, więc nie musimy skalować cech\n",
    "float_transformer = Pipeline([\n",
    "    ('float', ColumnRemover(0.9999, 0.99, 10))])\n",
    "\n",
    "col_transformer = ColumnTransformer([\n",
    "    ('int_pipe', int_transformer, make_column_selector(dtype_include=np.int64)),\n",
    "    ('float_pipe', float_transformer, make_column_selector(dtype_include=np.float64))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2adacbb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   Predicted 0 |   Predicted 1 |\n",
      "|---------------+---------------|\n",
      "|         23557 |             3 |\n",
      "|             0 |           940 |\n",
      "\n",
      "accuracy:              0.9999\n",
      "precision:             0.9968\n",
      "recall:                1.0\n",
      "f1:                    0.9984\n",
      "roc_auc_discrete:      0.9999\n",
      "roc_auc_continuous:    1.0\n"
     ]
    }
   ],
   "source": [
    "clf = Pipeline([\n",
    "    ('preprocessing', col_transformer),\n",
    "    ('model', DecisionTreeClassifier(random_state=42, class_weight='balanced'))])\n",
    "\n",
    "clf.fit(x_train, y_train)\n",
    "show_scores(clf, x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f72ac9e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   Predicted 0 |   Predicted 1 |\n",
      "|---------------+---------------|\n",
      "|         13852 |           547 |\n",
      "|           530 |            71 |\n",
      "\n",
      "accuracy:              0.9282\n",
      "precision:             0.1149\n",
      "recall:                0.1181\n",
      "f1:                    0.1165\n",
      "roc_auc_discrete:      0.5401\n",
      "roc_auc_continuous:    0.5401\n"
     ]
    }
   ],
   "source": [
    "show_scores(clf, x_eval, y_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a007cd1",
   "metadata": {},
   "source": [
    "Porównując wyniki na zbiorze treningowym i testowym, widzimy, że drzewo decyzyjne się przeucza. Rozwiązaniem może być wybór maksymalnej głębokości i redukcja kolumn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea8241b",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "78970e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "int_transformer = Pipeline([\n",
    "    ('int', ColumnRemover(0.9995, 0.99, 1)),\n",
    "    ('one_hot', OneHotEncoder(handle_unknown='ignore', sparse=False, dtype='int64'))])\n",
    "\n",
    "float_transformer = Pipeline([\n",
    "    ('float', ColumnRemover(0.9999, 0.99, 10))])\n",
    "\n",
    "col_transformer = ColumnTransformer([\n",
    "    ('int_pipe', int_transformer, make_column_selector(dtype_include=np.int64)),\n",
    "    ('float_pipe', float_transformer, make_column_selector(dtype_include=np.float64))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9d077f08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   Predicted 0 |   Predicted 1 |\n",
      "|---------------+---------------|\n",
      "|         23560 |             0 |\n",
      "|             3 |           937 |\n",
      "\n",
      "accuracy:              0.9999\n",
      "precision:             1.0\n",
      "recall:                0.9968\n",
      "f1:                    0.9984\n",
      "roc_auc_discrete:      0.9984\n",
      "roc_auc_continuous:    1.0\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators=1000)\n",
    "\n",
    "clf = Pipeline([\n",
    "    ('preprocessing', col_transformer),\n",
    "    ('model', rf)])\n",
    "\n",
    "clf.fit(x_train, y_train)\n",
    "show_scores(clf, x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d9b7f1",
   "metadata": {},
   "source": [
    "Patrząc na same wyniki na zbiorze treningowym, możemy podejrzewać, że model bardzo się przeuczył. Sprawdźmy jak sprawa wygląda przy zbiorze testowym."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "779104c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   Predicted 0 |   Predicted 1 |\n",
      "|---------------+---------------|\n",
      "|         14370 |            29 |\n",
      "|           593 |             8 |\n",
      "\n",
      "accuracy:              0.9585\n",
      "precision:             0.2162\n",
      "recall:                0.0133\n",
      "f1:                    0.0251\n",
      "roc_auc_discrete:      0.5056\n",
      "roc_auc_continuous:    0.8017\n"
     ]
    }
   ],
   "source": [
    "show_scores(clf, x_eval, y_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca56618",
   "metadata": {},
   "source": [
    "Nasze podejrzenia okazały się prawdziwe, gdyż model zdecydowanie gorzej poradził sobie na zbiorze testowym. Trzeba zastosować dodatkowe parametry, aby uniknąć przeuczania się modelu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65e957e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
